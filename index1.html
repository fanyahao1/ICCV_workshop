<!DOCTYPE html>
<!-- saved from url=(0043)https://fanyahao1.github.io/ICCV_workshop/# -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Multi-Scenarios Humanoid Locomotion Workshop @ICCV2025</title>
  <link rel="stylesheet" href="./css/style.css">
<style>hcfy-result.__hcfy__result__loaded__.__hcfy__result__both__{border: 1px dotted}</style></head>

<body>
  <div class="nav">
    <div class="nav-container">
      <img src="./img/iccv-navbar-logo.svg" alt="ICCV Logo" style="margin-top: 0px; border-radius: 0px; position: absolute; left: 1em; max-height: 40px;">
      <a href="https://fanyahao1.github.io/ICCV_workshop/#">Home</a>
      <a href="https://fanyahao1.github.io/ICCV_workshop/#intro">Introduction</a>
      <a href="https://fanyahao1.github.io/ICCV_workshop/#OpenSources">Open Sources</a>
      <a href="https://fanyahao1.github.io/ICCV_workshop/#rules">Rules</a>
      <a href="https://fanyahao1.github.io/ICCV_workshop/#tracks">Tracks</a>
      <!-- <a href="https://fanyahao1.github.io/ICCV_workshop/#call">Call for Papers</a> -->
    </div>
  </div>

  <div class="title-container">
    <div class="overlay"></div>
    <div class="content" style="text-align: center; margin: 20px">
      <h1>Multi-Terrain Humanoid Locomotion Challenge in Human-Robot-Scene Interaction and Collaboration</h1>
      <div class="subtitle">
        <a href="https://iccv.thecvf.com/">ICCV 2025</a> Workshop
      </div>
      <div class="subtitle">Oct 20th (Afternoon), 2025</div>
      <div class="subtitle">Honolulu, Hawai'i</div>
    </div>
  </div>

  <div class="container">

    <div class="section" id="intro">
      <h2>Introduction</h2>
      <p>Robust locomotion across diverse, unstructured terrain is a fundamental challenge for humanoid robots, critical for their deployment in real-world environments like homes, disaster zones, construction sites, and natural landscapes. As humanoid robots move beyond controlled labs to work alongside humans, they must navigate complex, varying surfaces reliably, efficiently, and safely to be truly useful collaborators within dynamic human-scene ecosystems.</p>
      <p>As a key component of the <a href="https://human-robot-scene.github.io/">Human-Robot-Scene Interaction and Collaboration Workshop @ICCV2025</a> , this Multi-Terrain Humanoid Locomotion Challenge aims to accelerate progress in this vital area. We provide participants with a challenging suite of diverse terrains implemented within the high-performance Isaac Gym simulator, alongside a foundational training platform. This enables researchers to focus on developing and training advanced locomotion control policies for simulated humanoids.</p>
      <p>Our challenge focuses on advancing multi-terrain locomotion capabilities, encompassing, but not limited to:</p>
      <ul>
        <li>Developing Robust Control Policies: Training locomotion strategies (e.g., reinforcement learning, optimization-based control) that enable humanoids to stably traverse a wide variety of simulated terrains (e.g., slopes, stairs, rubble, uneven ground, slippery surfaces).</li>
        <li>Adaptation and Generalization: Creating policies capable of adapting locomotion gaits and strategies in real-time to unseen terrain variations or disturbances encountered during traversal.</li>
        <li>Benchmarking Performance: Establishing clear metrics and evaluation protocols to rigorously measure and compare the stability, efficiency (e.g., energy consumption), speed, and robustness of locomotion strategies across the provided terrain suite.</li>
      </ul>
      <p>This challenge provides a crucial stepping stone, enabling rapid iteration and innovation in simulation, ultimately contributing to the development of humanoid robots capable of seamless and reliable movement in the complex, multi-terrain environments they are destined to share with humans.</p> 
  </div>


    <div class="section" id="OpenSources">
      <h2>Open Source</h2>
      <p>
        Our Open-source Terrain Benchmark is accessible in <a href="https://github.com/shiki-ta/Humanoid-Terrain-Bench">this repository</a>. You are required to use this repository to participate in the challenge using whichever robot you have, by default we provided Unitree H1_2、G1 and Fourier GR1T2 training code in Isaac Gym. Customized robots are also welcome, as long as they can be trained in Isaac Gym, but you may provide the Isaac Gym deploy config.
      </p>
    <h3>Resources</h3>
    <ul>
      <li><a href="https://github.com/shiki-ta/Humanoid-Terrain-Bench/tree/master/challenging_terrain">Terrain Suite</a>: A diverse set of terrains implemented in Isaac Gym, including slopes, stairs, rubble, uneven ground, and slippery surfaces.</li>
      <li><a href="https://github.com/shiki-ta/Humanoid-Terrain-Bench/tree/master/legged_gym">Training Platform</a>: A foundational training platform for developing and testing locomotion control policies.</li>
      <li><a href="https://github.com/shiki-ta/Humanoid-Terrain-Bench/blob/master/README.md">Documentation</a>: Comprehensive documentation to guide participants in using the benchmark effectively.</li>
      <li><a href="https://arxiv.org/abs/2505.18780" target="_blank">Reference paper</a>: We highly recommend you to read this paper to understand how to train a humanoid locomotion policy from scratch for multi-terrain crossing in Isaac Gym.</li>
      <li>Evaluation Metrics: Standardized metrics for assessing locomotion performance across different terrains.</li>
    </ul>
    </div>

    <div class="section" id="rules">
      <h2>Rules</h2>
      <ol>
        <li><strong>Eligibility:</strong> Participation is open to academic, industry, and independent teams worldwide. Each team may submit only one entry.</li>
        <li><strong>Robot Requirements:</strong> You may use any simulated humanoid robot compatible with Isaac Gym. Default support is provided for Unitree H1_2, G1, and Fourier GR1T2. Custom robots are welcome but must include deployment configs for Isaac Gym and contact the organizer in advance to adapt the code properly.</li>
        <li><strong>Submission:</strong> Teams must submit both their trained locomotion policy and the model inference code according to the provided submission guidelines. All entries must be reproducible.</li>
        <li><strong>Evaluation:</strong> All policies will be tested on a standardized suite of terrains and disturbances. Performance will be measured via completion rate and success rate. Speed is not a mandatory indicator, but in the same situation, faster speed will be better.</li>
        <li><strong>Fair Play:</strong> Use of cheating techniques, hard-coding the test environment, or exploiting simulator bugs is strictly prohibited.</li>
        <li><strong>Final Decisions:</strong> The organizing committee reserves the right to make final decisions regarding rule interpretation and winner selection.</li>
      </ol>
    </div>

    <div class="section" id="tracks">
      <h2>Tracks</h2>
      <p> We have designed different tracks to evaluate the different performance of robots.All terrain used for evaluation comes from the <a href="https://github.com/shiki-ta/Humanoid-Terrain-Bench/blob/master/challenging_terrain/terrain_base/single_terrain.py">single terrain model</a> of the terrain module provided in the code

      <ol>
        <li><strong>Robustness Evaluation:</strong> To evaluate the robustness of the robot, we will extend the size of the terrain several times. Taking stair terrain as an example, we will expand the 5-10 steps used during training to 20 or even more steps to test the robustness of the robot.</li>
        
        <div style="text-align:center; margin-bottom:10px;">
          <img src="./img/Robustness.png" alt="Terrain Suite Overview" style="max-width:75%;">
          <div style="color: #777;">Stair for Robustness evaluation</div>
        </div>


        <li><strong>Extreme Evaluation:</strong> To assess the upper limits of locomotion strategies, we conduct extreme terrain evaluations. For example, the height of stairs will be set significantly higher than in typical scenarios. These challenging conditions are designed to test the maximal capability of the trained humanoid policies, revealing potential failure modes and identifying areas for further improvement..</li>

        
        <div style="text-align:center; margin-bottom:10px;">
          <img src="./img/Extreme.png" alt="Terrain Suite Overview" style="max-width:75%;">
          <div style="color: #777;">Stair for Extreme evaluation</div>
        </div>

        <li><strong>Generalization Evaluation:</strong> To evaluate the generalization performance of locomotion strategies, we create complex scenarios by combining multiple types of terrain within single environments. By requiring robots to transition smoothly and adaptively between different terrain elements—such as stairs, slopes, rubble, and narrow bridges—we can more rigorously assess their ability to generalize learned skills and robustly handle a variety of unseen or mixed conditions in real-world environments.</li>
        
        <div style="text-align:center; margin-bottom:10px;">
          <img src="./img/simple.png" alt="Terrain Suite Overview">
          <div style="color: #777;"> Simple for Generalization Evaluation</div>
          <img src="./img/normal.png" alt="Terrain Suite Overview">
          <div style="color: #777;"> Normal for Generalization Evaluation</div>
          <img src="./img/designhard.png" alt="Terrain Suite Overview">
          <div style="color: #777;"> Normal1 for Generalization Evaluation</div>
          <img src="./img/hard.png" alt="Terrain Suite Overview">
          <div style="color: #777;"> Hard for Generalization Evaluation</div>
          <img src="./img/extreme-hard.png" alt="Terrain Suite Overview">
          <div style="color: #777;"> Challenging for Generalization Evaluation</div>
        </div>
      </ol>
    </div>
    
    <div class="section" id="call">
      <h2>Call for Papers</h2>
      <p>
        We invite submissions of <strong>long papers</strong> (up to 8 pages excluding references) and <strong>short papers</strong> (up to 4 pages excluding references) that explore humanoid locomotion.
      </p>

      <h3>Paper Topics</h3>
      <p>
        Topics of interests include, but are not limited to:
        </p><ul>
          <li><strong>Dexterous Grasping Algorithms</strong>: Developing grasping planning and execution algorithms capable of handling various object shapes, materials, and poses.</li>
          <li><strong>Human Grasping Behavior Modeling</strong>: Advancing techniques for capturing and modeling human grasping actions and interactions.</li>
          <li><strong>Knowledge Transfer</strong>: Leveraging insights from human grasping behaviors to inform the development of dexterous robotic hands.</li>
          <li><strong>Cross-Hand Retargeting</strong>: Techniques for adapting grasping strategies across different hand morphologies (e.g., from human hands to various robotic hand configurations).</li>
          <li><strong>Tactile Sensing and Control</strong>: Exploring tactile feedback in dexterous grasping to achieve fine force control and object manipulation.</li>
          <li><strong>Benchmarking and Metrics</strong>: Establishing meaningful benchmarks and metrics to assess advancements in dexterous grasping and human-robot-scene interaction.</li>
        </ul>
      <p></p>
      <p><strong>Disclaimer:</strong> This Open-source repository is provided for academic research purposes only and may not be used for commercial applications. By using this repository, you agree to use it solely for non-commercial academic research.</p>

      <h3>Submission Guidelines</h3>
      <p>As a dedicated sub-area within the Human-Robot-Scene Interaction and Collaboration Workshop @ICCV2025, please submit your papers to the <a href="https://human-robot-scene.github.io/" target="_blank">Human-Robot-Scene Interaction and Collaboration Workshop</a>.</p>
    </div>

  <footer>
    <p>© Multi-Scenarios Humanoid Locomotion Challenge in Human-Robot-Scene Interaction and Collaboration Workshop @ICCV2025</p>
  </footer>

  </div>
  </div>
</body>